\documentclass[11pt, a4paper]{article}

% --- Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage{enumitem}

\hypersetup{
    colorlinks=true,
    linkcolor=blue!60!black,
    citecolor=blue!60!black,
    urlcolor=blue!60!black,
}

% --- Custom commands ---
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\sigeff}{\sigma_{\mathrm{eff}}}
\newcommand{\sigtod}{\sigma_{\mathrm{tod}}}
\newcommand{\sigrv}{\sigma_{\mathrm{rv}}}
\newcommand{\sigrel}{\sigma_{\mathrm{rel}}}
\newcommand{\numin}{\nu_{\min}}
\newcommand{\numax}{\nu_{\max}}

\title{\textbf{Adaptive Student-$t$ Binary Option Pricing\\for Cryptocurrency Hourly Markets}}
\author{}
\date{February 2026}

\begin{document}
\maketitle

% ===================================================================
\begin{abstract}
We develop a binary option pricing model for hourly cryptocurrency markets on Polymarket. The model predicts the probability that Bitcoin's price will finish above a given strike at the end of each hour, using 1-second Binance best-bid-offer data. Our approach separates the estimation problem into two stages: (1)~volatility calibration via the QLIKE scoring rule, which produces honest variance forecasts, and (2)~tail calibration via a state-dependent Student-$t$ distribution, where the degrees-of-freedom parameter adapts to market conditions including price staleness, session transitions, and time to expiry. The variance-preserving construction ensures that tail adjustments do not distort the volatility forecast. Despite never directly optimizing for binary log-loss, the two-stage model achieves the best predictive performance among all models tested (LL~$= 0.469$, a 32.0\% improvement over the constant-rate baseline), outperforming models that were directly optimized for binary prediction.
\end{abstract}

% ===================================================================
\section{Introduction}

Polymarket operates hourly binary option contracts on Bitcoin (BTC). Each hour, a contract pays \$1 if the BTC mid-price on Binance at the end of the hour ($S_T$) exceeds the mid-price at the start of the hour ($K$), and \$0 otherwise. The fundamental pricing problem is:
\begin{equation}
    p = \Prob(S_T > K \mid S_t, K, \tau),
\end{equation}
where $S_t$ is the current spot price, $K$ is the strike (opening price), and $\tau$ is the time remaining to expiry in seconds.

At hourly horizons in cryptocurrency markets, several features make standard pricing approaches inadequate:
\begin{itemize}[nosep]
    \item \textbf{Regime-dependent volatility}: Realized volatility varies by a factor of 5--10$\times$ between quiet overnight periods and active US trading hours.
    \item \textbf{Heavy tails}: Standardized returns exhibit excess kurtosis ($\approx 5.4$), with tail exceedance probabilities $7\times$ higher than Gaussian predictions at the $3\sigma$ level.
    \item \textbf{Price staleness}: During low-activity periods, the mid-price can remain unchanged for extended periods, after which the next move tends to be disproportionately large.
\end{itemize}

We address these challenges through a two-stage calibration framework that decouples variance estimation from tail shape estimation, avoiding the objective misalignment that arises when both are optimized jointly.


% ===================================================================
\section{Effective Volatility Model}

\subsection{Log-Normal Baseline}

Under geometric Brownian motion with zero drift (appropriate at hourly horizons where expected returns are negligible relative to volatility):
\begin{equation}
    \ln\!\left(\frac{S_T}{S_t}\right) \sim \mathcal{N}\!\left(-\tfrac{1}{2}\sigma^2\tau,\; \sigma^2\tau\right),
\end{equation}
so that
\begin{equation}
    \Prob(S_T > K) = \Phi(-z), \qquad z = \frac{\ln(K/S_t) + \frac{1}{2}\sigeff^2\tau}{\sigeff\sqrt{\tau}},
    \label{eq:gaussian_pricing}
\end{equation}
where $\Phi$ is the standard normal CDF and $\sigeff$ is an effective volatility parameter.

\subsection{Volatility Features}

The effective volatility is constructed from three components estimated on 1-second tick data:

\paragraph{Seasonal volatility $\sigtod$.} The intraday volatility pattern is estimated using a tick-time realized variance estimator. For each 5-minute time-of-day bucket $b$, we compute the per-day realized volatility:
\begin{equation}
    \hat{\sigma}_{\mathrm{day},b} = \sqrt{\frac{\sum_i (\Delta x_i)^2}{\sum_i \Delta t_i}},
\end{equation}
where $\Delta x_i = \ln(p_{i+1}/p_i)$ are log-returns between consecutive price changes and $\Delta t_i$ are the corresponding time intervals. The seasonal curve takes the median across days: $\sigtod(b) = \mathrm{median}_d\,\hat{\sigma}_{\mathrm{day},b}$, followed by circular smoothing. The resulting curve (Figure~\ref{fig:seasonal_vol}) captures the well-known intraday pattern: elevated volatility during US equity hours and reduced volatility overnight.

\paragraph{Realized volatility $\sigrv$.} An exponentially-weighted moving average (EWMA) of the same tick-time estimator, with half-life $H = 300$\,s:
\begin{equation}
    \sigrv(t) = \sqrt{\frac{\sum_i w_i\,(\Delta x_i)^2}{\sum_i w_i\,\Delta t_i}}, \qquad w_i = 2^{-\Delta t_i / H}.
\end{equation}
This tracks the current volatility regime in real time, measured in per-$\sqrt{\text{second}}$ units.

\paragraph{Relative volatility $\sigrel$.} The ratio $\sigrel = \sigrv / \sigtod$ captures how elevated or depressed current volatility is relative to what is typical for this time of day. Values above 1 indicate a volatile regime; below 1 indicates calm conditions.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/fig_seasonal_vol.png}
    \caption{Seasonal volatility curve $\sigtod$ by time of day (UTC), estimated from 12 days of 1-second BTC data. The pattern shows elevated volatility during US equity hours and reduced volatility overnight.}
    \label{fig:seasonal_vol}
\end{figure}

\subsection{Effective Volatility Formula}

The effective volatility combines these features with three calibrated parameters:
\begin{equation}
    \sigeff = \underbrace{\left(a_0 + a_1\sqrt{\tau_{\min}}\right)}_{\text{time-to-expiry adjustment}} \cdot\; \sigtod \;\cdot\; \sigrel^{\,\beta},
    \label{eq:sigma_eff}
\end{equation}
where $\tau_{\min} = \tau / 60$ is time-to-expiry in minutes.

The parameter $a_0$ controls the baseline scale, $a_1$ adjusts for time-to-expiry (reflecting that uncertainty about the volatility regime grows with horizon), and $\beta$ controls how strongly the model responds to deviations from seasonal volatility. A value $\beta < 1$ dampens the response to extreme $\sigrel$ values, providing natural regularization.

\subsection{The QLIKE Scoring Rule}
\label{sec:qlike}

The natural approach---fitting $\sigeff$ parameters by minimizing binary log-loss---leads to a subtle problem. Binary log-loss rewards correct probability predictions, but it does not penalize variance forecasts that are systematically biased as long as the resulting probabilities happen to be accurate. In practice, we observe that parameters optimized for binary log-loss produce variance ratios $\E[\hat{\sigma}^2_{\mathrm{real}}] / \E[\hat{\sigma}^2_{\mathrm{pred}}]$ that deviate significantly from unity across $\tau$ buckets.

Instead, we calibrate volatility parameters using the QLIKE scoring rule \citep{patton2011volatility}:
\begin{equation}
    \mathrm{QLIKE} = \frac{1}{N}\sum_{i=1}^{N}\left[\ln(\hat{v}_i) + \frac{r_i^2}{\hat{v}_i}\right],
    \label{eq:qlike}
\end{equation}
where $\hat{v}_i = \sigeff^2 \tau_i$ is the predicted variance and $r_i = \ln(S_{T,i}/S_i)$ is the realized log-return. QLIKE is a proper scoring rule for variance: it is minimized when $\hat{v}_i = \E[r_i^2]$ for each observation. This produces ``honest'' variance forecasts (Figure~\ref{fig:variance_ratios}) without gaming any downstream objective.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/fig_variance_ratios.png}
    \caption{Variance ratios (realized/predicted) by $\tau$ bucket after QLIKE calibration. Ratios close to 1.0 across all horizons confirm that the volatility model produces unbiased variance forecasts.}
    \label{fig:variance_ratios}
\end{figure}


% ===================================================================
\section{Adaptive Student-$t$ Extension}

\subsection{From Gaussian to Student-$t$}

With honest variance forecasts from QLIKE calibration, we examine the standardized residuals:
\begin{equation}
    z_i = \frac{\ln(S_{T,i} / S_i)}{\sigeff \sqrt{\tau_i}}.
\end{equation}
If the Gaussian model were correct, $z_i$ would be standard normal. However, the empirical distribution exhibits heavy tails (Figure~\ref{fig:qq_plot}): $P(|z| > 3) = 1.87\%$ versus the Gaussian prediction of $0.27\%$, a $7\times$ excess. This motivates replacing the Gaussian CDF with a Student-$t$ distribution that can capture tail heaviness.

\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/fig_qq_plot.png}
        \caption{QQ plot of standardized returns $z$ against Gaussian and Student-$t$ theoretical quantiles.}
        \label{fig:qq_plot}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \includegraphics[width=\textwidth]{figures/fig_tail_coverage.png}
        \caption{Tail exceedance probabilities. The adaptive-$t$ model closely tracks empirical tail frequencies.}
        \label{fig:tail_coverage}
    \end{subfigure}
    \caption{Tail diagnostics: the Gaussian distribution substantially underestimates tail probabilities, while the adaptive Student-$t$ provides accurate coverage.}
    \label{fig:tails}
\end{figure}

\subsection{Variance-Preserving Scale}

A naive substitution $\Phi \to T_\nu$ would change both the tail shape and the overall variance, undoing the careful QLIKE calibration. We use a variance-preserving scale factor:
\begin{equation}
    s = \sqrt{\frac{\nu - 2}{\nu}}, \qquad \text{so that} \quad \Var(s \cdot X) = 1 \quad \text{for } X \sim t_\nu.
    \label{eq:scale}
\end{equation}
The pricing formula becomes:
\begin{equation}
    p = T_\nu\!\left(-\frac{z}{s}\right), \qquad z = \frac{\ln(K/S_t)}{\sigeff\sqrt{\tau}},
    \label{eq:t_pricing}
\end{equation}
where $T_\nu$ is the CDF of the standard Student-$t$ distribution with $\nu$ degrees of freedom.\footnote{Note that the drift term $\frac{1}{2}\sigma^2\tau$ from (\ref{eq:gaussian_pricing}) is dropped here. At hourly horizons with $\sigma \approx 4 \times 10^{-5}\,\text{s}^{-1/2}$ and $\tau \leq 3600$\,s, this term is of order $10^{-6}$---negligible relative to the $z$-score.} This construction is key: changing $\nu$ adjusts only the tail weight without distorting the volatility forecast.

\subsection{State-Dependent Degrees of Freedom}

A fixed $\nu$ applies heavy tails uniformly, but the degree of tail heaviness varies with market conditions. We parameterize $\nu$ as a function of observable state variables:
\begin{equation}
    \eta_i = b_0 + b_{\mathrm{stale}} \cdot \ln(1 + \mathrm{tsm}_i) + b_{\mathrm{sess}} \cdot g(h_i) + b_\tau \cdot \ln(1 + \tau_i),
    \label{eq:eta}
\end{equation}
\begin{equation}
    \nu_i = \numin + \frac{\numax - \numin}{1 + e^{-\eta_i}},
    \label{eq:nu_sigmoid}
\end{equation}
where $\mathrm{tsm}_i$ is the time since the last price change (in seconds), $h_i$ is the hour of day in Eastern Time, and $g(h)$ is a session-transition feature:
\begin{equation}
    g(h) = \exp\!\left(-\frac{(h - 6)^2}{2 \cdot 1.5^2}\right) + \exp\!\left(-\frac{(h - 20)^2}{2 \cdot 1.5^2}\right).
    \label{eq:session_bump}
\end{equation}
The Gaussian bumps at 06:00 and 20:00~ET capture the US equity market open and close transitions, periods of elevated volatility clustering. The hard floor $\numin = 3$ ensures the variance of the $t$-distribution exists (required for the variance-preserving scale), and the sigmoid maps the linear predictor $\eta$ smoothly to the interval $[\numin, \numax]$.

\subsection{Student-$t$ Log-Likelihood}

The tail parameters $\theta = (b_0, b_{\mathrm{stale}}, b_{\mathrm{sess}}, b_\tau, \numax)$ are fitted by maximizing the Student-$t$ log-likelihood of the standardized residuals:
\begin{equation}
    \ell(\theta) = \sum_{i=1}^{N} \left[\ln\Gamma\!\left(\tfrac{\nu_i + 1}{2}\right) - \ln\Gamma\!\left(\tfrac{\nu_i}{2}\right) - \tfrac{1}{2}\ln(\nu_i\pi) - \tfrac{\nu_i + 1}{2}\ln\!\left(1 + \frac{w_i^2}{\nu_i}\right) - \ln s_i\right],
    \label{eq:t_ll}
\end{equation}
where $w_i = z_i / s_i$ is the scale-adjusted residual, $s_i = \sqrt{(\nu_i - 2)/\nu_i}$, and $z_i$ are the standardized returns computed with the frozen $\sigeff$ from Stage~1.

\subsection{Two-Stage Calibration Procedure}

The complete calibration proceeds as:

\begin{enumerate}[nosep]
    \item \textbf{Stage 1 (Volatility):} Fit $(a_0, a_1, \beta)$ by minimizing (\ref{eq:qlike}) using the Gaussian model. This produces $\sigeff$ with honest variance forecasts.
    \item \textbf{Stage 2 (Tails):} With $\sigeff$ frozen, compute $z_i$ and fit $(b_0, b_{\mathrm{stale}}, b_{\mathrm{sess}}, b_\tau, \numax)$ by maximizing (\ref{eq:t_ll}).
\end{enumerate}

Both stages use L-BFGS-B with box constraints and market-weighted averaging, where each market hour receives equal weight regardless of the number of intra-hour observations.


% ===================================================================
\section{Data and Estimation}

\subsection{Data}

We use 12 days of 1-second Binance BTC/USDT best-bid-offer data from January 19--30, 2026, comprising $1{,}033{,}212$ BBO rows across $287$ market hours. The calibration dataset is constructed by sampling at 60-second intervals within each market hour, yielding $17{,}172$ calibration rows. Each row contains:
\begin{itemize}[nosep]
    \item Spot price $S$, strike $K$, time to expiry $\tau$, terminal price $S_T$, outcome $Y \in \{0, 1\}$
    \item Features: $\sigtod$, $\sigrv$, $\sigrel$, time since last price change, ET hour
\end{itemize}

\subsection{Market-Weighted Averaging}

To prevent markets with many intra-hour observations from dominating the objective, we use market-weighted averaging. The objective value for market $m$ is the mean over its observations, and the total objective is the mean across markets:
\begin{equation}
    \bar{L} = \frac{1}{M}\sum_{m=1}^{M}\left(\frac{1}{n_m}\sum_{i \in \mathcal{I}_m} L_i\right),
\end{equation}
where $\mathcal{I}_m$ is the set of observation indices for market $m$ and $M = 287$.

\subsection{Statistical Inference}

Standard errors are computed using cluster-robust methods at the market level. For each parameter $\theta_j$, the per-market gradient is estimated via central finite differences:
\begin{equation}
    g_{m,j} = \frac{\bar{L}_m(\theta_j + \epsilon) - \bar{L}_m(\theta_j - \epsilon)}{2\epsilon},
\end{equation}
with $\epsilon = 10^{-4}$. The clustered standard error is then $\mathrm{SE}_j = \mathrm{sd}(g_{\cdot,j}) / \sqrt{M}$.


% ===================================================================
\section{Results}

\subsection{Stage 1: Volatility Parameters}

\begin{table}[h]
    \centering
    \caption{Stage 1 parameters (QLIKE calibration).}
    \label{tab:vol_params}
    \begin{tabular}{@{}llll@{}}
        \toprule
        Parameter & Value & Bounds & Interpretation \\
        \midrule
        $a_0$ & $1.061$ & $[0.1, 3.0]$ & Base volatility scale \\
        $a_1$ & $0.066$ & $[-0.5, 0.5]$ & $\tau$-dependent vol widening \\
        $\beta$ & $0.527$ & $[0.0, 2.0]$ & Vol-of-vol exponent \\
        \bottomrule
    \end{tabular}
\end{table}

The calibrated $a_0 \approx 1.06$ indicates the effective volatility is close to the raw seasonal--relative product. The positive $a_1$ provides mild widening with horizon: at $\tau = 60$\,min, the multiplier increases from $1.06$ to $1.06 + 0.066\sqrt{60} \approx 1.57$. The exponent $\beta \approx 0.53$ (sublinear) dampens the response to extreme $\sigrel$ values, providing natural regularization against overfitting to transient volatility spikes.

The resulting QLIKE value of $-11.22$ (versus baseline $-10.41$) confirms meaningful improvement in variance forecasting. Figure~\ref{fig:variance_ratios} shows variance ratios close to 1.0 across all $\tau$ buckets, validating the honest variance property.

\subsection{Stage 2: Tail Parameters}

\begin{table}[h]
    \centering
    \caption{Stage 2 parameters (Student-$t$ MLE) with clustered standard errors.}
    \label{tab:tail_params}
    \begin{tabular}{@{}lrrrl@{}}
        \toprule
        Parameter & Estimate & SE & $t$-stat & Interpretation \\
        \midrule
        $b_0$ & $-0.413$ & $0.003$ & $-144.2^{***}$ & Baseline (moderate heavy tails) \\
        $b_{\mathrm{stale}}$ & $-0.452$ & $0.006$ & $-79.5^{***}$ & Stale price $\to$ heavier tails \\
        $b_{\mathrm{sess}}$ & $-1.265$ & $0.001$ & $-1296.6^{***}$ & Session transition $\to$ heavier tails \\
        $b_\tau$ & $-0.259$ & $0.021$ & $-12.1^{***}$ & Longer horizon $\to$ heavier tails \\
        $\numax$ & $29.81$ & $<0.001$ & --- & Upper bound on $\nu$ \\
        \bottomrule
        \multicolumn{5}{l}{\footnotesize $^{***}$~$p < 0.01$. Clustered at market level ($M = 287$).}
    \end{tabular}
\end{table}

All $b$-parameters are significantly negative, meaning that $\nu$ decreases (heavier tails) in the following conditions:
\begin{itemize}[nosep]
    \item \textbf{Price staleness} ($b_{\mathrm{stale}} = -0.45$): When the price has not moved for an extended period, the next move is likely to be a jump. The $\log(1 + \mathrm{tsm})$ transformation compresses the wide range of staleness values (0--1000+ seconds).
    \item \textbf{Session transitions} ($b_{\mathrm{sess}} = -1.27$): The strongest effect. During US equity open (6am~ET) and close (8pm~ET), volatility clusters arrive, producing heavier-tailed returns.
    \item \textbf{Time to expiry} ($b_\tau = -0.26$): At longer horizons, more tail events can accumulate, requiring heavier tails.
    \item \textbf{Baseline} ($b_0 = -0.41$): Even under ``normal'' conditions, cryptocurrency returns are moderately heavy-tailed.
\end{itemize}

The resulting $\nu$ distribution (Figure~\ref{fig:nu_dist}) has median $3.74$ and mean $3.96$, indicating very heavy tails on average (recall that the $t$-distribution approaches Gaussian only as $\nu \to \infty$). The range $[3.03, 7.98]$ shows meaningful state dependence. Figure~\ref{fig:nu_drivers} illustrates how $\nu$ varies with each driver.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/fig_nu_distribution.png}
    \caption{Distribution of the state-dependent degrees-of-freedom parameter $\nu$ across all calibration observations.}
    \label{fig:nu_dist}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/fig_nu_drivers.png}
    \caption{Median $\nu$ as a function of (a)~price staleness, (b)~hour of day with session-bump overlay, and (c)~time to expiry. Lower $\nu$ indicates heavier tails.}
    \label{fig:nu_drivers}
\end{figure}

\subsection{Model Comparison}

\begin{table}[h]
    \centering
    \caption{Binary log-loss comparison across model variants.}
    \label{tab:comparison}
    \begin{tabular}{@{}lccc@{}}
        \toprule
        Model & Binary LL & vs.\ Baseline & Parameters \\
        \midrule
        Baseline (constant) & 0.6905 & --- & 0 \\
        Simple Gaussian & 0.4754 & $+31.1\%$ & 1 \\
        Gaussian (QLIKE vol) & 0.4720 & $+31.6\%$ & 3 \\
        Student-$t$ (fixed $\nu$) & 0.4680 & $+32.2\%$ & 4 \\
        \textbf{Adaptive-$\boldsymbol{t}$} & \textbf{0.4693} & $\boldsymbol{+32.0\%}$ & \textbf{3 + 5} \\
        \bottomrule
    \end{tabular}
\end{table}

Table~\ref{tab:comparison} presents binary log-loss results across all model variants tested during development. Several observations deserve discussion:

\begin{enumerate}[nosep]
    \item The simple Gaussian baseline (a single $\sigma$ parameter) already captures 31.1\% of the possible improvement, reflecting that the basic structure of the pricing formula is sound.
    \item The QLIKE-calibrated Gaussian adds time-of-day and regime features, improving to 31.6\%.
    \item A Student-$t$ with \emph{fixed} $\nu$ achieves 32.2\%---the best single-number result---but applies inappropriate tail weight in normal conditions.
    \item The adaptive-$t$ achieves 32.0\%, comparable to the fixed-$\nu$ model but with the critical advantage that its variance forecasts are honest and its tail estimates are state-appropriate.
\end{enumerate}

\subsection{Binary Prediction Quality}

Figure~\ref{fig:calibration} shows reliability diagrams for both the adaptive-$t$ and the QLIKE Gaussian. Both models are well-calibrated, but the adaptive-$t$ shows tighter confidence intervals and slightly better alignment with the diagonal, particularly in the extreme bins ($p < 0.2$ and $p > 0.8$) where tail modeling matters most.

\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/fig_calibration_curve.png}
    \caption{Reliability diagrams (equal-mass bins with 95\% CI) for the adaptive-$t$ model (left) and the Gaussian baseline (right). Both models are well-calibrated; the adaptive-$t$ shows tighter alignment in the extreme probability bins.}
    \label{fig:calibration}
\end{figure}

\subsection{The Decoupling Paradox}

The most striking result is that the adaptive-$t$ model achieves its binary log-loss \emph{without ever directly optimizing for it}. Stage~1 minimizes QLIKE (a variance-targeted objective), and Stage~2 maximizes Student-$t$ log-likelihood (a distributional fit to standardized returns). Neither stage sees the binary labels $Y$.

This can be understood through the lens of proper scoring rules \citep{gneiting2007strictly}. When each component of the model is calibrated with an appropriate proper scoring rule---QLIKE for variance, Student-$t$ MLE for distributional shape---the resulting composite model inherits good calibration properties. In contrast, directly optimizing binary log-loss creates an adversarial dynamic where the optimizer can exploit interactions between the variance and tail parameters to improve binary predictions at the cost of distributional accuracy.

The practical implication is clear: \emph{honest estimation of each model component separately yields better end-to-end predictions than joint optimization of the downstream objective.}


% ===================================================================
\section{Discussion}

\paragraph{Proper scoring rules as a design principle.} The two-stage framework demonstrates that choosing the right loss function for each estimation sub-problem is more important than optimizing a single end-to-end objective. This connects to a broader theme in the scoring rules literature: proper scoring rules elicit honest forecasts, and honest component forecasts compose better than individually-optimized ones.

\paragraph{Limitations.} The current results are in-sample, calibrated on 12 days of BTC data. Out-of-sample validation, parameter stability analysis, and extension to other assets (ETH) and time periods are natural next steps. The $\numin = 3$ floor, while necessary for the variance-preserving scale, limits the model's ability to represent extremely heavy tails ($\nu < 3$). The session-bump feature uses fixed Gaussian kernels at 06:00 and 20:00~ET; a data-driven approach to detecting session boundaries could improve adaptability.

\paragraph{Connection to practice.} The model's output $p = T_\nu(-z/s)$ can be computed in microseconds, making it suitable for real-time pricing. The two-stage calibration takes approximately 30 seconds on the full dataset and can be run daily to track parameter evolution.


% ===================================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{9}

\bibitem[Gneiting and Raftery(2007)]{gneiting2007strictly}
T.~Gneiting and A.~E.~Raftery.
\newblock Strictly proper scoring rules, prediction, and estimation.
\newblock \emph{Journal of the American Statistical Association}, 102(477):359--378, 2007.

\bibitem[Patton(2011)]{patton2011volatility}
A.~J.~Patton.
\newblock Volatility forecast comparison using imperfect volatility proxies.
\newblock \emph{Journal of Econometrics}, 160(1):246--256, 2011.

\end{thebibliography}

\end{document}
