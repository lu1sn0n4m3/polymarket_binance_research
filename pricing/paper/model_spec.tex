\documentclass[11pt, a4paper]{article}

% --- Packages ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{natbib}

\hypersetup{
    colorlinks=true,
    linkcolor=blue!60!black,
    citecolor=blue!60!black,
    urlcolor=blue!60!black,
}

% --- Theorem environments ---
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{assumption}{Assumption}

% --- Custom commands ---
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\sigEff}{\sigma_{\mathrm{eff}}}
\newcommand{\sigTod}{\sigma_{\mathrm{tod}}}
\newcommand{\sigRv}{\sigma_{\mathrm{rv}}}
\newcommand{\sigRel}{\sigma_{\mathrm{rel}}}

\title{\textbf{A Variance-First Framework for Binary Option Pricing\\under Heavy-Tailed Innovations}\\[0.5em]
\large Model Specification and Theoretical Foundations}
\author{}
\date{February 2026}

\begin{document}
\maketitle

% ===================================================================
\begin{abstract}
We provide a complete mathematical specification for a two-stage binary option pricing model applicable to
high-frequency cryptocurrency markets. The model decomposes the pricing problem into (i) honest conditional
variance forecasting via the QLIKE proper scoring rule and (ii) a variance-preserving Student-$t$
tail overlay that improves tail coverage without distorting the variance forecast. The final model
has five parameters: four for the variance model ($c$, $\alpha$, $k_0$, $k_1$)
and one for the tail shape ($\nu$). The variance model incorporates horizon-dependent shrinkage
toward the seasonal baseline, reverting the regime-responsiveness term at longer horizons where
transient volatility deviations are less informative. We state all assumptions explicitly, prove the key properties (QLIKE properness,
variance preservation under the $t$-overlay, and the decoupling guarantee), develop a rigorous
diagnostic framework based on conditional calibration of normalized squared returns, and specify the
complete pricer construction from raw tick data to real-time probability output. The document is self-contained
and purely theoretical; empirical results are reported separately.
\end{abstract}

\tableofcontents
\newpage

% ===================================================================
\section{Introduction and Problem Statement}
\label{sec:intro}

\subsection{The binary pricing problem}

Consider an asset with price process $(S_t)_{t \geq 0}$ observed in continuous time. A binary option contract
is defined by a strike price $K > 0$ and an expiry time $T > 0$. The contract pays 1 if $S_T > K$ and 0 otherwise.
At any time $t < T$, the fair price of this contract under the physical measure is:
\begin{equation}
    p_t := \Prob(S_T > K \mid \F_t),
    \label{eq:binary_price}
\end{equation}
where $(\F_t)_{t \geq 0}$ is the natural filtration of the price process and $\Prob$ denotes the physical
probability measure.

\begin{remark}
In standard option pricing theory, one prices under a risk-neutral measure $\mathbb{Q}$. We work under the
physical measure $\Prob$ for two reasons: (i)~the contracts considered settle against the realized spot price,
so the pricing problem is equivalent to probability forecasting under $\Prob$; (ii)~at hourly horizons, the
expected return under $\Prob$ is negligible relative to the standard deviation (see Assumption~\ref{ass:zero_drift}),
so the distinction between $\Prob$ and $\mathbb{Q}$ is immaterial for binary pricing.
\end{remark}

\subsection{Log-return representation}

Define the terminal log-return and the log-strike distance:
\begin{align}
    r_{t,T} &:= \ln\!\left(\frac{S_T}{S_t}\right), \label{eq:log_return} \\
    k_t &:= \ln\!\left(\frac{K}{S_t}\right). \label{eq:log_strike}
\end{align}
Then $S_T > K$ if and only if $r_{t,T} > k_t$, and the pricing problem becomes:
\begin{equation}
    p_t = \Prob(r_{t,T} > k_t \mid \F_t) = 1 - F_{r_{t,T} \mid \F_t}(k_t),
    \label{eq:p_as_cdf}
\end{equation}
where $F_{r_{t,T} \mid \F_t}$ is the conditional CDF of $r_{t,T}$ given $\F_t$.

\subsection{Design philosophy}

To evaluate \eqref{eq:p_as_cdf}, one must specify the full conditional distribution of $r_{t,T}$. We decompose
this problem into two independent sub-problems:
\begin{enumerate}[nosep]
    \item \textbf{Location and scale:} Forecast the conditional first and second moments of $r_{t,T}$.
    \item \textbf{Shape:} Choose a parametric innovation distribution whose tails match the empirical residuals.
\end{enumerate}
The key design principle is that each sub-problem is calibrated with its own \emph{proper scoring rule}, ensuring
that the solution to one sub-problem does not distort the other. We call this the \emph{variance-first} approach.


% ===================================================================
\section{Assumptions}
\label{sec:assumptions}

We state the assumptions under which the model operates. These are motivated by the structure of
high-frequency cryptocurrency price data at horizons of seconds to one hour.

\begin{assumption}[Zero conditional drift]
\label{ass:zero_drift}
The conditional expected log-return satisfies
\begin{equation}
    \E[r_{t,T} \mid \F_t] \approx 0
\end{equation}
in the sense that $|\E[r_{t,T} \mid \F_t]| \ll \sqrt{\Var(r_{t,T} \mid \F_t)}$ for all $t$ and $T-t \leq 3600$\,s.
\end{assumption}

\begin{remark}
For a continuous semimartingale with bounded drift $\mu$ and volatility $\sigma$, the expected log-return over
horizon $\tau = T - t$ is $\mu\tau - \frac{1}{2}\sigma^2\tau$, while the standard deviation is $\sigma\sqrt{\tau}$.
The ratio is $|\mu\tau - \frac{1}{2}\sigma^2\tau| / (\sigma\sqrt{\tau}) = O(\sqrt{\tau})$, which vanishes as $\tau \to 0$.
At $\tau = 3600$\,s and typical cryptocurrency volatility $\sigma \sim 4 \times 10^{-5}\,\text{s}^{-1/2}$,
this ratio is of order $10^{-3}$. We therefore treat the conditional mean as zero.
\end{remark}

\begin{assumption}[Finite conditional second moment]
\label{ass:finite_second}
The conditional second moment exists and is positive:
\begin{equation}
    0 < \E[r_{t,T}^2 \mid \F_t] < \infty \qquad \text{a.s.\ for all } t < T.
\end{equation}
\end{assumption}

\begin{assumption}[Conditional independence of scale and shape]
\label{ass:scale_shape}
There exists a measurable function $v_t(\tau) > 0$ (the \emph{conditional variance forecast}) and a random
variable $\varepsilon_t$ such that:
\begin{equation}
    r_{t,T} = \sqrt{v_t(\tau)}\,\varepsilon_t,
    \label{eq:decomp}
\end{equation}
where $\varepsilon_t$ has $\E[\varepsilon_t \mid \F_t] = 0$, $\Var(\varepsilon_t \mid \F_t) = 1$, and the
conditional distribution of $\varepsilon_t$ given $\F_t$ depends on $\F_t$ only through a finite-dimensional
sufficient statistic.
\end{assumption}

\begin{remark}
Assumption~\ref{ass:scale_shape} is the central structural assumption. It asserts that the conditional
distribution of $r_{t,T}$ factors into a scale component $\sqrt{v_t(\tau)}$ (which captures all the
conditional variance dynamics) and a shape component $\varepsilon_t$ (which captures the standardized
distributional form). This factorization is exact for any location-scale family and holds approximately
for a much wider class of models. The assumption permits the two-stage calibration: Stage~1 targets
$v_t(\tau)$ and Stage~2 targets the distribution of $\varepsilon_t$.
\end{remark}

\begin{assumption}[Stationarity of the innovation distribution]
\label{ass:stationary_innovation}
The conditional distribution of $\varepsilon_t$ given $\F_t$ is stationary over the calibration window,
in the sense that its shape parameters (e.g.\ degrees of freedom $\nu$) are constant or depend only on
slowly varying state variables.
\end{assumption}

\begin{assumption}[QLIKE regularity]
\label{ass:qlike_regularity}
The parametric family $\{v_t(\tau;\theta) : \theta \in \Theta\}$ satisfies:
\begin{enumerate}[nosep, label=(\alph*)]
    \item $\Theta \subset \R^d$ is compact.
    \item $\theta \mapsto v_t(\tau;\theta)$ is continuous and bounded away from zero and infinity on $\Theta$.
    \item There exists $\theta^* \in \Theta$ such that $v_t(\tau;\theta^*) = \E[r_{t,T}^2 \mid \F_t]$ a.s.
\end{enumerate}
\end{assumption}


% ===================================================================
\section{Stage 1: Conditional Variance Model}
\label{sec:stage1}

\subsection{Variance-first decomposition}

Under Assumptions~\ref{ass:zero_drift}--\ref{ass:scale_shape}, the conditional variance of the terminal
log-return equals its conditional second moment:
\begin{equation}
    \Var(r_{t,T} \mid \F_t) = \E[r_{t,T}^2 \mid \F_t] - \underbrace{\E[r_{t,T} \mid \F_t]^2}_{\approx\, 0}
    \approx \E[r_{t,T}^2 \mid \F_t].
    \label{eq:var_equals_second_moment}
\end{equation}
We model this quantity directly as $v_t(\tau;\theta)$, a parametric function of observable features
and remaining time $\tau = T - t$. The variance has units of (log-return)$^2$, matching $r_{t,T}^2$.

\subsection{Input features}
\label{sec:features}

The variance forecast is constructed from observable quantities computed on high-frequency tick data.

\subsubsection{Tick-time realized variance}

Let $(p_j, t_j)_{j=1,2,\ldots}$ denote the sequence of mid-price observations at times $t_j$ where the
mid-price actually changed (i.e.\ $p_j \neq p_{j-1}$). Define the log-returns and time intervals:
\begin{equation}
    \Delta x_j := \ln(p_j / p_{j-1}), \qquad \Delta t_j := t_j - t_{j-1}.
    \label{eq:tick_returns}
\end{equation}

The \emph{tick-time realized variance} over a window $[a,b]$ is:
\begin{equation}
    \widehat{\mathrm{RV}}_{[a,b]} := \frac{\sum_{j:\,t_j \in [a,b]} (\Delta x_j)^2}
                                          {\sum_{j:\,t_j \in [a,b]} \Delta t_j}.
    \label{eq:rv_tick}
\end{equation}
This estimator normalizes by elapsed real time (not the number of ticks), yielding a per-second variance
estimate. The tick-time construction avoids the bias that arises in calendar-time realized variance when the
price is stale: if the mid-price does not change for an interval, no artificial zero returns are included
in the sum.

\begin{remark}
The estimator \eqref{eq:rv_tick} is consistent for the integrated variance of the continuous-time price
process under standard regularity conditions (finite activity jumps, bounded volatility). Its key advantage
over calendar-time realized variance is robustness to irregular price arrivals, which are prevalent in
24-hour cryptocurrency markets.
\end{remark}

\subsubsection{Seasonal volatility curve}

Partition the 24-hour day into $B = 288$ time-of-day buckets (five-minute buckets). For each
bucket $b \in \{1, \ldots, B\}$ and each calendar day $d$ in the calibration sample, compute the tick-time
realized volatility:
\begin{equation}
    \hat{\sigma}_{d,b} := \sqrt{\widehat{\mathrm{RV}}_{[a_{d,b},\, a_{d,b}+\Delta]}},
    \label{eq:sigma_day_bucket}
\end{equation}
where $[a_{d,b},\, a_{d,b}+\Delta]$ is the time interval corresponding to bucket $b$ on day $d$ and
$\Delta$ is the bucket width. The seasonal volatility curve is defined as the cross-day median:
\begin{equation}
    \sigTod(b) := \mathrm{median}_{d}\,\hat{\sigma}_{d,b},
    \label{eq:sigma_tod}
\end{equation}
Before aggregation, the squared tick returns $(\Delta x_j)^2$ are winsorized at the $(1-q)$th percentile
(default $q = 0.01$) to limit the influence of microstructure outliers (e.g.\ flash wicks). The seasonal
curve is then circularly smoothed using a uniform (box) kernel of width $2W+1$ buckets on the periodic
domain $[0, 24)$\,h (default $W = 3$, yielding a 35-minute window) and floored at $\sigTod(b) \geq 10^{-10}$
to prevent numerical issues in buckets with negligible tick activity.

\begin{remark}
The median is used instead of the mean for robustness to outlier days (news events, flash crashes). The
uniform circular smoothing respects the periodicity of the time-of-day domain. The resulting curve $\sigTod(b)$
is a non-parametric estimate of the ``typical'' volatility at each time of day.
\end{remark}

\subsubsection{Weekday--weekend split}

Cryptocurrency markets trade continuously, but the intraday volatility pattern differs between weekdays
and weekends (lower overall activity, different time-of-day peaks). We estimate separate seasonal curves
$\sigTod^{\mathrm{wd}}(b)$ and $\sigTod^{\mathrm{we}}(b)$ on weekday and weekend subsets of the
calibration window, and dispatch by calendar day at pricing time.

\subsubsection{Integrated seasonal volatility}

A point lookup $\sigTod(b_t)$ uses only the current bucket's volatility. For a contract with remaining
life $\tau$, the relevant quantity is the average variance over the entire interval $[t, t+\tau]$. Define
the \emph{integrated seasonal volatility}:
\begin{equation}
    \bar{\sigma}_{\mathrm{tod}}^2(t,\tau) := \frac{1}{\tau}\int_{t}^{t+\tau} \sigTod(s)^2\,ds,
    \label{eq:sigma_tod_integrated}
\end{equation}
where $\sigTod(s)$ is evaluated at the bucket containing time $s$ (with weekday/weekend dispatch).
This is computed by summing $\sigTod(b)^2 \cdot (\text{overlap seconds})$ across the 5-minute buckets
that span $[t, t+\tau]$.

\begin{remark}
The integrated form is important near intraday transitions. For example, when pricing at 13:25~UTC
(a quiet period) with $\tau = 35$~min, the remaining life extends into 14:00~UTC (U.S.\@ equity open),
where volatility is $2$--$3\times$ higher. A point estimate would miss this; the integrated form
captures it. Throughout the remainder of this document, $\sigTod$ refers to the integrated quantity
$\bar{\sigma}_{\mathrm{tod}}(t,\tau)$ unless otherwise noted.
\end{remark}

\subsubsection{Real-time realized volatility}

The current volatility regime is tracked by an exponentially weighted moving average (EWMA) of the
tick-time realized variance:
\begin{equation}
    \sigRv(t) := \sqrt{\frac{\sum_{j:\,t_j \leq t} w_j\,(\Delta x_j)^2}
                             {\sum_{j:\,t_j \leq t} w_j\,\Delta t_j}},
    \qquad w_j := 2^{-(t - t_j)/H},
    \label{eq:sigma_rv}
\end{equation}
where $H > 0$ is the half-life parameter (in seconds; default $H = 600$). The exponential weighting ensures
that the estimator responds to regime changes with a characteristic time of $H / \ln 2$ seconds.
The EWMA accumulators are initialized using the first $\min(100, \lfloor N/10 \rfloor)$ tick
observations (where $N$ is the total number of ticks) to avoid dependence on an arbitrary prior
during cold-start.

\subsubsection{Relative regime level}

The relative volatility measures how the current regime compares to the seasonal norm over the
contract's remaining life:
\begin{equation}
    \sigRel(t,\tau) := \frac{\sigRv(t)}{\bar{\sigma}_{\mathrm{tod}}(t,\tau)},
    \label{eq:sigma_rel}
\end{equation}
where $\bar{\sigma}_{\mathrm{tod}}(t,\tau)$ is the integrated seasonal volatility (\ref{eq:sigma_tod_integrated}).
Values $\sigRel > 1$ indicate an elevated regime; $\sigRel < 1$ indicates suppressed volatility
relative to the seasonal expectation.

\begin{remark}
The use of the integrated seasonal denominator makes $\sigRel$ horizon-dependent: the same
$\sigRv(t)$ yields different $\sigRel$ values for contracts with different remaining lives.
This is deliberate---a contract expiring before a known volatility spike should have a different
regime assessment than one expiring after it.
\end{remark}

\subsection{Parametric variance model}

We combine the features into a multiplicative variance forecast with state-dependent shrinkage:
\begin{equation}
    \boxed{
    v_t(\tau;\theta) = c^2 \;\cdot\; \sigTod(t,\tau)^2 \;\cdot\; \tau^{\alpha}
    \;\cdot\; \bigl[\,m_t(\tau) + (1 - m_t(\tau))\,\sigRel(t,\tau)^{2}\,\bigr],
    }
    \label{eq:v_model}
\end{equation}
where $\sigTod(t,\tau) \equiv \bar{\sigma}_{\mathrm{tod}}(t,\tau)$ is the integrated seasonal volatility
\eqref{eq:sigma_tod_integrated}, and the \emph{shrinkage weight} $m_t(\tau) \in (0,1)$ is defined by:
\begin{equation}
    \boxed{
    m_t(\tau) := \sigma\!\bigl(k_0 + k_1 \ln\tau\bigr),
    }
    \label{eq:shrinkage}
\end{equation}
with $\sigma(x) = 1/(1+e^{-x})$ the sigmoid function.
The full parameter vector is $\theta = (c, \alpha, k_0, k_1)$.

\begin{remark}[Interpretation of the shrinkage factor]
\label{rem:v_interpretation}
\hfill

The factor $f_t := m_t + (1 - m_t)\,\sigRel^{2}$ interpolates between two extremes:
\begin{itemize}[nosep]
    \item $m_t = 0$: Full regime pass-through. $f_t = \sigRel^{2}$, recovering the pure
        power-law model.
    \item $m_t = 1$: Full seasonal reversion. $f_t = 1$, so $v_t = c^2\,\sigTod^2\,\tau^\alpha$---the
        model ignores $\sigRel$ entirely and prices from the seasonal curve alone.
\end{itemize}
The two sigmoid inputs control the shrinkage:
\begin{itemize}[nosep]
    \item $k_0$: Baseline shrinkage level.
    \item $k_1 \ln\tau$: Horizon-dependent shrinkage. When $k_1 > 0$, longer horizons induce stronger
        reversion to seasonal---consistent with the intuition that short-lived volatility spikes
        matter less for longer-dated contracts.
\end{itemize}
This two-parameter form is deliberately parsimonious. The shrinkage depends only on the horizon
$\tau$, not on $\sigRel$ itself, so that the regime signal enters the variance exclusively through
the power-law $\sigRel^{2}$.
\end{remark}

\begin{remark}[Remaining components]
\hfill
\begin{itemize}[nosep]
    \item $c^2$: Overall variance scale. Absorbs systematic biases in the input estimators.
    \item $\sigTod(t,\tau)^2$: Integrated intraday seasonal pattern. Captures the $5$--$10\times$
        variance ratio between active and quiet periods, averaged over the contract's remaining life.
    \item $\sigRel(t,\tau)^{2}$: Regime responsiveness. The squared term gives linear variance
        response to deviations from the seasonal norm, modulated by the shrinkage weight.
    \item $\tau^{\alpha}$: Horizon scaling. $\alpha = 1$ for a pure diffusion; $\alpha < 1$ for
        mean-reverting volatility; $\alpha > 1$ for regime-switching or trending volatility.
\end{itemize}
\end{remark}

\begin{remark}[Positivity and multiplicative structure]
The multiplicative structure \eqref{eq:v_model} ensures that $v_t(\tau;\theta) > 0$ for all parameter
values in the feasible region, without requiring explicit positivity constraints. The shrinkage factor
$f_t \in (0, \infty)$ is always positive since $m_t \in (0,1)$ and $\sigRel^{2} > 0$.
\end{remark}

\subsection{Calibration via the QLIKE scoring rule}
\label{sec:qlike}

\subsubsection{The QLIKE objective}

Given calibration observations $(r_i, \tau_i, \mathbf{x}_i)_{i=1}^N$, where $r_i$ is the realized log-return,
$\tau_i$ is the time to expiry, and $\mathbf{x}_i$ collects all features, define:
\begin{equation}
    \hat{v}_i := v_{t_i}(\tau_i;\theta).
\end{equation}
The QLIKE scoring rule is:
\begin{equation}
    \boxed{
    \mathrm{QLIKE}(\theta) := \frac{1}{N}\sum_{i=1}^{N} L_{\mathrm{Q}}(r_i^2, \hat{v}_i),
    \qquad L_{\mathrm{Q}}(y, v) := \ln v + \frac{y}{v}.
    }
    \label{eq:qlike}
\end{equation}

\subsubsection{Properness of QLIKE}

\begin{definition}[Proper scoring rule for the second moment]
\label{def:proper}
A scoring rule $S(y, v)$ for a non-negative target $y := r^2$ and a forecast $v > 0$ is \emph{proper}
if for all distributions of $y$ with finite mean $\mu = \E[y]$:
\begin{equation}
    \E[S(y, \mu)] \leq \E[S(y, v)] \qquad \text{for all } v > 0,
\end{equation}
with equality if and only if $v = \mu$. It is \emph{strictly proper} if the inequality is strict for $v \neq \mu$.
\end{definition}

\begin{proposition}[QLIKE is strictly proper]
\label{prop:qlike_proper}
The scoring rule $L_{\mathrm{Q}}(y, v) = \ln v + y/v$ is strictly proper for the conditional mean of $y = r^2$.
That is, for any random variable $y > 0$ with $\E[y] = \mu \in (0,\infty)$:
\begin{equation}
    \E[L_{\mathrm{Q}}(y, v)] \text{ is uniquely minimized at } v = \mu.
\end{equation}
\end{proposition}

\begin{proof}
Fix any $v > 0$. Compute:
\begin{equation}
    \E[L_{\mathrm{Q}}(y,v)] = \ln v + \frac{\mu}{v}.
\end{equation}
This is a function of $v$ alone (since $\E[y] = \mu$ is fixed). Taking the derivative:
\begin{equation}
    \frac{d}{dv}\left(\ln v + \frac{\mu}{v}\right) = \frac{1}{v} - \frac{\mu}{v^2} = \frac{v - \mu}{v^2}.
\end{equation}
Setting this to zero gives $v^* = \mu$. The second derivative is:
\begin{equation}
    \frac{d^2}{dv^2}\left(\ln v + \frac{\mu}{v}\right) = -\frac{1}{v^2} + \frac{2\mu}{v^3},
\end{equation}
which at $v = \mu$ equals $-1/\mu^2 + 2/\mu^2 = 1/\mu^2 > 0$, confirming a strict minimum.
Since $\ln v + \mu/v \to +\infty$ as $v \to 0^+$ or $v \to +\infty$, the minimum is global and unique.
\end{proof}

\begin{corollary}[Honest variance forecasts]
\label{cor:honest}
If the parametric family $\{v_t(\tau;\theta)\}$ is rich enough to represent $\E[r_{t,T}^2 \mid \F_t]$
(Assumption~\ref{ass:qlike_regularity}(c)), then QLIKE minimization produces:
\begin{equation}
    \hat{v}_i(\hat{\theta}) = \E[r_i^2 \mid \F_{t_i}] \qquad \text{a.s.\ at the population optimum,}
\end{equation}
where $\hat{\theta}$ is the QLIKE minimizer. We call this the \emph{honest variance} property.
\end{corollary}

\begin{remark}[Why not binary log-loss for variance calibration?]
\label{rem:why_not_logloss}
Binary log-loss is a proper scoring rule for \emph{probabilities}, not for \emph{variances}. When
variance parameters are fitted by minimizing binary log-loss, the optimizer can exploit the nonlinear
map $v \mapsto p$ (through the Gaussian or Student-$t$ CDF) to improve probability predictions at the
cost of systematic variance bias. Concretely, the optimizer may inflate $v$ in regimes where $k_t \approx 0$
(near at-the-money), because a wider distribution assigns probability closer to 0.5, which has low binary
log-loss when the true probability is near 0.5. QLIKE prevents this by targeting variance directly.
\end{remark}

\subsubsection{Market-weighted QLIKE}
\label{sec:market_weighted}

In practice, observations are sampled within market hours (contracts), and markets with more intra-hour
observations should not dominate the objective. Let $\mathcal{M} = \{1, \ldots, M\}$ index distinct markets,
and let $\mathcal{I}_m$ be the set of observation indices belonging to market $m$, with $|\mathcal{I}_m| = n_m$.
The market-weighted QLIKE is:
\begin{equation}
    \overline{\mathrm{QLIKE}}(\theta) := \frac{1}{M}\sum_{m=1}^{M} \left(\frac{1}{n_m}\sum_{i \in \mathcal{I}_m}
    L_{\mathrm{Q}}(r_i^2, \hat{v}_i)\right).
    \label{eq:qlike_weighted}
\end{equation}
This gives each market equal influence, regardless of the number of within-market observations.


% ===================================================================
\section{Stage 1 Diagnostics: Conditional Calibration}
\label{sec:diagnostics}

\subsection{The normalized squared return}

Define the \emph{variance ratio} or \emph{normalized squared return}:
\begin{equation}
    u_i := \frac{r_i^2}{\hat{v}_i}.
    \label{eq:u_def}
\end{equation}

\begin{proposition}[Conditional calibration criterion]
\label{prop:u_criterion}
If $\hat{v}_i = \E[r_i^2 \mid \F_{t_i}]$ (i.e.\ the variance forecast is honest), then:
\begin{equation}
    \E[u_i \mid \F_{t_i}] = 1 \qquad \text{a.s.}
    \label{eq:u_one}
\end{equation}
Conversely, if $\E[u_i \mid X_i] \neq 1$ for some $\F_{t_i}$-measurable variable $X_i$, then the variance
forecast is conditionally biased with respect to $X_i$.
\end{proposition}

\begin{proof}
Immediate from $\E[u_i \mid \F_{t_i}] = \E[r_i^2 \mid \F_{t_i}] / \hat{v}_i = \hat{v}_i / \hat{v}_i = 1$.
For the converse, if $\E[u_i \mid X_i] \neq 1$ and $X_i$ is $\F_{t_i}$-measurable, then by the tower property
$\E[u_i \mid X_i] = \E[\E[u_i \mid \F_{t_i}] \mid X_i]$, so $\E[u_i \mid \F_{t_i}] = 1$ a.s.\ would imply
$\E[u_i \mid X_i] = 1$, a contradiction.
\end{proof}

\subsection{Practical diagnostic procedure}

Since $\F_{t_i}$ is infinite-dimensional, we test the condition $\E[u_i \mid X_i] \approx 1$ for a small set
of low-dimensional state variables $X_i$ observable at time $t_i$:

\begin{enumerate}[nosep]
    \item \textbf{Horizon:} $X = \tau$. Detects misspecification of the exponent $\alpha$ in $\tau^\alpha$.
    \item \textbf{Regime:} $X = \sigRel$. Detects misspecification of the shrinkage parameters
        $(k_0, k_1)$ or the EWMA half-life $H$.
    \item \textbf{Staleness:} $X = \Delta t_{\mathrm{last}}$, the elapsed time (in seconds) since the
        mid-price last changed. Detects whether stale-quote effects bias the variance forecast.
    \item \textbf{Hour of day:} $X = h_t$ (Eastern Time). Detects residual seasonality not captured
        by $\sigTod$.
\end{enumerate}

For each scalar diagnostic variable $X$, form equal-mass bins $B_1, \ldots, B_J$ and compute:
\begin{equation}
    \bar{u}(B_j) := \frac{1}{|B_j|}\sum_{i \in B_j} u_i,
    \qquad \text{with cluster-robust SE at the market level.}
    \label{eq:u_bin}
\end{equation}
A well-calibrated model produces $\bar{u}(B_j) \approx 1$ uniformly across bins.

\subsection{Minimal correction principle}

Each diagnostic failure should be addressed by adjusting a single structural parameter rather than
adding high-dimensional features:
\begin{itemize}[nosep]
    \item $\E[u \mid \tau]$ drifts with $\tau$: adjust $\alpha$.
    \item $\E[u \mid \sigRel]$ slopes: adjust the shrinkage coefficients $(k_0, k_1)$
        or filter half-life $H$.
    \item $\E[u \mid \Delta t_{\mathrm{last}}]$ deviates: investigate EWMA decay behavior
        during stale-quote periods.
    \item $\E[u \mid h_t]$ deviates by hour of day: re-estimate or re-smooth $\sigTod$.
\end{itemize}
The guiding principle is parsimony: each conditional-bias failure points to a specific model
mechanism, and the fix is a one-knob structural adjustment rather than an ex-post patch.


% ===================================================================
\section{The Gaussian Pricer}
\label{sec:gaussian_pricer}

Under the simplifying assumption that $\varepsilon_t \sim \mathcal{N}(0,1)$ (the standardized innovation
is Gaussian), the decomposition \eqref{eq:decomp} gives:
\begin{equation}
    r_{t,T} \mid \F_t \sim \mathcal{N}(0,\, v_t(\tau)),
\end{equation}
where we have used Assumption~\ref{ass:zero_drift} to set the mean to zero. The binary probability is then:
\begin{equation}
    \boxed{
    p_t^{\mathrm{Gauss}} = \Phi\!\left(-\frac{k_t}{\sqrt{v_t(\tau)}}\right)
    = \Phi\!\left(-\frac{\ln(K/S_t)}{\sqrt{v_t(\tau;\theta)}}\right),
    }
    \label{eq:p_gauss}
\end{equation}
where $\Phi$ is the standard normal CDF.

\begin{remark}[Drift term]
Under the geometric Brownian motion model $dS/S = \mu\,dt + \sigma\,dW$, the log-return has mean
$(\mu - \frac{1}{2}\sigma^2)\tau$ and the correct $z$-score includes a $\frac{1}{2}\sigma^2\tau$ correction.
Under Assumption~\ref{ass:zero_drift}, this correction is of order $\sigma^2\tau \sim 10^{-6}$ for hourly
horizons and is absorbed into the zero-mean approximation. We therefore omit it.
\end{remark}

\begin{remark}[Symmetry property]
At the money ($S_t = K$, i.e.\ $k_t = 0$), equation \eqref{eq:p_gauss} gives $p_t = \Phi(0) = 0.5$,
independent of volatility. This is a consequence of the zero-drift and symmetric-innovation assumptions,
and it is a desirable property for a contract that pays on $S_T > K$.
\end{remark}


% ===================================================================
\section{Stage 2: Heavy-Tail Overlay via Student-$t$ Innovations}
\label{sec:stage2}

\subsection{Motivation}

If the Gaussian assumption were correct, the standardized residuals
\begin{equation}
    z_i := \frac{r_i}{\sqrt{\hat{v}_i}}
    \label{eq:z_resid}
\end{equation}
would be i.i.d.\ $\mathcal{N}(0,1)$. In practice, the empirical distribution of $z_i$ typically exhibits
excess kurtosis (heavier tails than Gaussian), even after careful variance calibration. This motivates
replacing the Gaussian innovation with a parametric family that can accommodate heavier tails.

\subsection{The Student-$t$ distribution}

\begin{definition}[Student-$t$ random variable]
A random variable $X$ follows a (standard) Student-$t$ distribution with $\nu > 0$ degrees of freedom,
written $X \sim t_\nu$, if its density is:
\begin{equation}
    f_\nu(x) = \frac{\Gamma\!\left(\frac{\nu+1}{2}\right)}{\sqrt{\nu\pi}\;\Gamma\!\left(\frac{\nu}{2}\right)}
    \left(1 + \frac{x^2}{\nu}\right)^{-(\nu+1)/2}.
    \label{eq:t_density}
\end{equation}
\end{definition}

Key properties relevant to our application:
\begin{itemize}[nosep]
    \item \textbf{Mean:} $\E[X] = 0$ for $\nu > 1$.
    \item \textbf{Variance:} $\Var(X) = \nu / (\nu - 2)$ for $\nu > 2$. Note: $\Var(X) > 1$ for all finite $\nu > 2$.
    \item \textbf{Kurtosis:} $\mathrm{Kurt}(X) = 3 + 6/(\nu - 4)$ for $\nu > 4$. Always exceeds the Gaussian
        value of 3.
    \item \textbf{Tail decay:} $f_\nu(x) \sim |x|^{-(\nu+1)}$ as $|x| \to \infty$ (polynomial tails),
        versus $e^{-x^2/2}$ for the Gaussian (exponential tails).
    \item \textbf{Gaussian limit:} $t_\nu \to \mathcal{N}(0,1)$ as $\nu \to \infty$.
\end{itemize}

\subsection{The variance-preservation problem}

A naive replacement of $\Phi$ by $T_\nu$ (the CDF of $t_\nu$) in \eqref{eq:p_gauss} would distort the
variance calibration. To see this, note that if $\varepsilon_t \sim t_\nu$, then:
\begin{equation}
    \Var(r_{t,T} \mid \F_t) = v_t(\tau) \cdot \Var(\varepsilon_t) = v_t(\tau) \cdot \frac{\nu}{\nu-2}
    \neq v_t(\tau),
\end{equation}
so the actual conditional variance would exceed the Stage~1 forecast by a factor of $\nu/(\nu-2) > 1$.
This is especially problematic for small $\nu$ (heavy tails): at $\nu = 4$, the inflation factor is 2.

\subsection{Variance-preserving scale}

\begin{proposition}[Variance-preserving Student-$t$ innovation]
\label{prop:var_preserve}
Let $X_\nu \sim t_\nu$ with $\nu > 2$. Define:
\begin{equation}
    s(\nu) := \sqrt{\frac{\nu - 2}{\nu}}, \qquad \varepsilon_t := s(\nu) \cdot X_\nu.
    \label{eq:vp_scale}
\end{equation}
Then:
\begin{enumerate}[nosep, label=(\roman*)]
    \item $\E[\varepsilon_t] = 0$.
    \item $\Var(\varepsilon_t) = 1$.
    \item $\varepsilon_t$ has heavier tails than $\mathcal{N}(0,1)$ for all finite $\nu > 2$.
    \item $\varepsilon_t \to \mathcal{N}(0,1)$ in distribution as $\nu \to \infty$.
\end{enumerate}
\end{proposition}

\begin{proof}
\hfill
\begin{enumerate}[nosep, label=(\roman*)]
    \item $\E[\varepsilon_t] = s(\nu) \cdot \E[X_\nu] = s(\nu) \cdot 0 = 0$ since $\nu > 2 > 1$.
    \item $\Var(\varepsilon_t) = s(\nu)^2 \cdot \Var(X_\nu) = \frac{\nu-2}{\nu} \cdot \frac{\nu}{\nu-2} = 1$.
    \item The density of $\varepsilon_t$ is $g(x) = \frac{1}{s(\nu)} f_\nu(x/s(\nu))$, which has polynomial
        tail decay $|x|^{-(\nu+1)}$, heavier than the Gaussian exponential decay for any finite $\nu$.
    \item As $\nu \to \infty$, $s(\nu) \to 1$ and $X_\nu \to \mathcal{N}(0,1)$ in distribution, so
        $\varepsilon_t = s(\nu) X_\nu \to \mathcal{N}(0,1)$.
        \qedhere
\end{enumerate}
\end{proof}

\begin{remark}[Why $\nu > 2$ is essential]
The variance-preserving scale $s(\nu) = \sqrt{(\nu-2)/\nu}$ requires $\nu > 2$ for $s(\nu)$ to be real and positive.
At $\nu = 2$, $s(\nu) = 0$ (degenerate), and for $\nu \leq 2$ the variance of $t_\nu$ is infinite, so the
concept of variance preservation is undefined. This imposes a hard floor $\nu_{\min} > 2$ on the degrees-of-freedom
parameter. In practice, we use $\nu_{\min} = 3$ to maintain a safety margin.
\end{remark}

\subsection{The Student-$t$ pricer}

Substituting the variance-preserving innovation \eqref{eq:vp_scale} into the decomposition \eqref{eq:decomp}:
\begin{equation}
    r_{t,T} = \sqrt{v_t(\tau)} \cdot s(\nu) \cdot X_\nu, \qquad X_\nu \sim t_\nu.
\end{equation}
Therefore:
\begin{equation}
    \Prob(r_{t,T} > k_t \mid \F_t) = \Prob\!\left(X_\nu > \frac{k_t}{s(\nu)\sqrt{v_t(\tau)}}\right)
    = 1 - T_\nu\!\left(\frac{k_t}{s(\nu)\sqrt{v_t(\tau)}}\right),
\end{equation}
where $T_\nu$ is the CDF of $t_\nu$. Using the symmetry $T_\nu(-x) = 1 - T_\nu(x)$:

\begin{equation}
    \boxed{
    p_t^{(t)} = T_\nu\!\left(-\frac{k_t}{s(\nu)\sqrt{v_t(\tau)}}\right)
    = T_\nu\!\left(-\frac{\ln(K/S_t)}{s(\nu)\,\sqrt{v_t(\tau;\theta)}}\right).
    }
    \label{eq:p_student}
\end{equation}

\begin{proposition}[Properties of the Student-$t$ pricer]
\label{prop:pricer_properties}
The pricing formula \eqref{eq:p_student} satisfies:
\begin{enumerate}[nosep, label=(\roman*)]
    \item \textbf{At-the-money symmetry:} $p_t^{(t)} = \frac{1}{2}$ when $S_t = K$ ($k_t = 0$), for all $\nu > 2$.
    \item \textbf{Boundary conditions:} $p_t^{(t)} \to 1$ as $S_t/K \to \infty$ and $p_t^{(t)} \to 0$ as $S_t/K \to 0$.
    \item \textbf{Monotonicity:} $p_t^{(t)}$ is strictly increasing in $S_t$ (holding other variables fixed).
    \item \textbf{Gaussian nesting:} $p_t^{(t)} \to p_t^{\mathrm{Gauss}}$ as $\nu \to \infty$.
    \item \textbf{Variance invariance:} The conditional variance $\Var(r_{t,T} \mid \F_t) = v_t(\tau)$ is
        independent of $\nu$. Changing $\nu$ affects only the tail shape.
    \item \textbf{Tail enrichment:} For $|k_t| \gg s(\nu)\sqrt{v_t(\tau)}$ (deep out-of-the-money),
        the Student-$t$ pricer assigns higher probability to extreme outcomes than the Gaussian pricer.
\end{enumerate}
\end{proposition}

\begin{proof}
Properties (i)--(iv) follow directly from the properties of $T_\nu$ (symmetry about zero, limits,
monotonicity, and Gaussian convergence). Property (v) is Proposition~\ref{prop:var_preserve}(ii).
For (vi), note that $1 - T_\nu(x) \sim c_\nu x^{-\nu}$ for large $x$ (polynomial decay), while
$1 - \Phi(x) \sim \phi(x)/x$ (exponential decay), so $[1 - T_\nu(x)] / [1 - \Phi(x)] \to \infty$
as $x \to \infty$.
\end{proof}

\subsection{Calibration of $\nu$}
\label{sec:nu_calibration}

With the Stage~1 variance $\hat{v}_i$ frozen, the standardized residuals $z_i = r_i / \sqrt{\hat{v}_i}$
are treated as realizations of the variance-preserving Student-$t$ innovation \eqref{eq:vp_scale}.
The density of $\varepsilon = s(\nu) X_\nu$ evaluated at $z$ is:
\begin{equation}
    g_\nu(z) = \frac{1}{s(\nu)}\,f_\nu\!\left(\frac{z}{s(\nu)}\right),
    \label{eq:vp_density}
\end{equation}
where $f_\nu$ is the standard $t_\nu$ density \eqref{eq:t_density}. The log-likelihood is:
\begin{equation}
    \boxed{
    \ell(\nu) = \sum_{i=1}^{N} \left[
    \ln\Gamma\!\left(\tfrac{\nu+1}{2}\right)
    - \ln\Gamma\!\left(\tfrac{\nu}{2}\right)
    - \tfrac{1}{2}\ln(\nu\pi)
    - \tfrac{\nu+1}{2}\ln\!\left(1 + \frac{w_i^2}{\nu}\right)
    - \ln s(\nu)
    \right],
    }
    \label{eq:t_loglik}
\end{equation}
where $w_i := z_i / s(\nu)$ is the rescaled residual. The maximum likelihood estimator is:
\begin{equation}
    \hat{\nu} = \arg\max_{\nu \in [3.01,\, 100]} \ell(\nu).
    \label{eq:nu_mle}
\end{equation}

\begin{remark}
The lower bound $\nu_{\min} = 3.01$ ensures that the variance of $t_\nu$ exists (required for the
variance-preserving scale) and that the kurtosis is finite (required for $\nu > 4$, though this is not
strictly necessary for the pricer). The single parameter $\nu$ controls both the kurtosis and the tail
decay rate, which is sufficient for a one-parameter shape extension.
\end{remark}

\subsection{Why the Student-$t$ is an appropriate choice}
\label{sec:t_justification}

The use of the Student-$t$ distribution for the standardized innovations merits explicit justification.

\subsubsection{Theoretical motivation: normal-variance mixtures}

The Student-$t$ distribution arises as a \emph{normal-variance mixture}. Specifically, if
\begin{equation}
    \varepsilon \mid V \sim \mathcal{N}(0, V), \qquad V \sim \mathrm{Inv\text{-}Gamma}\!\left(\tfrac{\nu}{2},\, \tfrac{\nu}{2}\right),
    \label{eq:normal_variance_mixture}
\end{equation}
then the marginal distribution of $\varepsilon$ is $t_\nu$ (up to the variance-preserving rescaling).
This representation admits a natural interpretation in our setting: the conditional variance $v_t(\tau)$
from Stage~1 captures the \emph{predictable} part of the variance, but there remains an \emph{unpredictable}
multiplicative perturbation $V$ of the local variance. If $V$ follows an inverse-gamma distribution
(a conjugate prior for the normal variance), the marginalized returns are Student-$t$.

In other words, the Student-$t$ model is the Bayesian predictive distribution under uncertainty about the true
local variance, given a diffusive price process with random variance. This is precisely the situation in
high-frequency markets where the instantaneous volatility fluctuates on time scales shorter than the forecast
horizon.

\subsubsection{Practical motivation: flexible kurtosis with unit variance}

Among unit-mean, unit-variance, symmetric, unimodal distributions commonly used in finance, the Student-$t$
family offers:
\begin{itemize}[nosep]
    \item A single ``knob'' $\nu$ that interpolates continuously between very heavy tails ($\nu \to 2^+$)
        and the Gaussian limit ($\nu \to \infty$).
    \item A closed-form CDF (via the regularized incomplete beta function), enabling microsecond-level evaluation.
    \item Kurtosis $3 + 6/(\nu - 4)$ for $\nu > 4$, providing a direct mapping between $\nu$ and the
        fourth-moment excess.
    \item Well-understood MLE properties: the score function and Fisher information for $\nu$ are available
        in closed form, and the MLE is consistent and asymptotically normal under standard regularity conditions.
\end{itemize}

\subsubsection{Comparison with alternatives}

Other heavy-tailed innovation distributions (generalized hyperbolic, normal-inverse Gaussian, stable)
offer additional flexibility but at the cost of more parameters and/or infinite variance (for stable
distributions with index $< 2$). The Student-$t$ is the minimal extension of the Gaussian that captures
excess kurtosis with a single parameter while maintaining finite variance.


% ===================================================================
\section{The Decoupling Guarantee}
\label{sec:decoupling}

The two-stage architecture rests on a formal guarantee that Stage~2 cannot corrupt Stage~1.

\begin{theorem}[Decoupling]
\label{thm:decoupling}
Let $v_t(\tau;\hat{\theta})$ be the Stage~1 variance forecast obtained by QLIKE minimization, and let
$\hat{\nu}$ be the Stage~2 degrees-of-freedom parameter obtained by maximizing \eqref{eq:t_loglik} with
$\hat{v}_i$ frozen. Then:
\begin{enumerate}[nosep, label=(\roman*)]
    \item The conditional second moment of $r_{t,T}$ under the Student-$t$ model is $v_t(\tau;\hat{\theta})$,
        independent of $\hat{\nu}$.
    \item The binary price $p_t^{(t)}$ given by \eqref{eq:p_student} depends on $\hat{\nu}$ only through
        the tail shape, not through the variance.
    \item If the true innovation distribution is $\varepsilon_t \sim \mathcal{N}(0,1)$, then $\hat{\nu} \to \infty$
        in probability as $N \to \infty$, and $p_t^{(t)} \to p_t^{\mathrm{Gauss}}$.
\end{enumerate}
\end{theorem}

\begin{proof}
\hfill
\begin{enumerate}[nosep, label=(\roman*)]
    \item Under the Student-$t$ model with variance-preserving scale:
        $\Var(r_{t,T} \mid \F_t) = v_t(\tau) \cdot \Var(s(\hat\nu) X_{\hat\nu}) = v_t(\tau) \cdot 1 = v_t(\tau)$,
        by Proposition~\ref{prop:var_preserve}(ii). This holds for all $\hat{\nu} > 2$.
    \item In \eqref{eq:p_student}, $\hat{\nu}$ enters only through $T_\nu$ and $s(\nu)$. The variance
        $v_t(\tau)$ is determined entirely by $\hat{\theta}$ and does not depend on $\hat{\nu}$.
    \item Under Gaussian innovations, the standardized residuals $z_i \sim \mathcal{N}(0,1)$. The likelihood
        ratio of $t_\nu$ versus $\mathcal{N}(0,1)$ converges to 1 for each observation as $\nu \to \infty$.
        By consistency of MLE, $\hat{\nu} \to \infty$ in probability.
        \qedhere
\end{enumerate}
\end{proof}

\begin{remark}[Why decoupling is more than convenience]
The decoupling guarantee has a practical consequence beyond mathematical elegance. It means that the variance
forecast can be independently validated (via the $u$-diagnostics in Section~\ref{sec:diagnostics}) without
regard to the tail parameter $\nu$. And conversely, $\nu$ can be recalibrated (e.g.\ after a regime change)
without re-running Stage~1. This modularity is essential for a system where the variance model may be retrained
daily but the tail parameter may be updated more or less frequently.
\end{remark}


% ===================================================================
\section{Proper Scoring Rules and Composability}
\label{sec:scoring_rules}

The two-stage architecture is grounded in the theory of proper scoring rules \citep{gneiting2007strictly}.
This section provides the relevant theoretical background.

\subsection{Definitions}

\begin{definition}[Scoring rule]
A \emph{scoring rule} is a function $S(y, q)$ that assigns a numerical score to a forecast $q$ given an
outcome $y$. Lower scores indicate better forecasts (loss convention).
\end{definition}

\begin{definition}[Proper scoring rule]
A scoring rule $S(y, q)$ is \emph{proper} with respect to a class of distributions $\mathcal{P}$ if for all
$P \in \mathcal{P}$:
\begin{equation}
    \E_P[S(Y, q^*)] \leq \E_P[S(Y, q)] \qquad \text{for all forecasts } q,
\end{equation}
where $q^* = q^*(P)$ is the ``truthful'' forecast associated with $P$. It is \emph{strictly proper} if
equality holds only when $q = q^*$.
\end{definition}

\subsection{QLIKE as a proper rule for the second moment}

The QLIKE rule $L_{\mathrm{Q}}(y, v) = \ln v + y/v$ is strictly proper for the mean of $y = r^2$
(Proposition~\ref{prop:qlike_proper}). It belongs to the Bregman divergence family generated by
$\phi(v) = -\ln v$:
\begin{equation}
    L_{\mathrm{Q}}(y, v) = \phi(v) + \phi'(v)(y - v) + C(y) = -\ln v + \frac{y - v}{v} + (1 + \ln y)
    = \ln v + \frac{y}{v} + \text{const}(y).
\end{equation}
The constant $C(y) = 1 + \ln y$ depends only on the observation and is irrelevant for optimization.

\subsection{Binary log-loss as a proper rule for probabilities}

The binary log-loss (or logarithmic scoring rule) for outcome $Y \in \{0,1\}$ and probability forecast $p$ is:
\begin{equation}
    L_{\mathrm{LL}}(Y, p) := -Y\ln p - (1-Y)\ln(1-p).
    \label{eq:logloss}
\end{equation}
This is strictly proper for the Bernoulli parameter $p^* = \Prob(Y=1)$: the expected loss
$\E[L_{\mathrm{LL}}(Y, p)] = -p^*\ln p - (1-p^*)\ln(1-p)$ is uniquely minimized at $p = p^*$
(by the Gibbs inequality).

\subsection{Composability of proper scoring rules}

\begin{proposition}[Informal composability]
\label{prop:composability}
If each component of a composite forecast is calibrated with a proper scoring rule that targets the
relevant functional of the distribution (variance for QLIKE, distributional shape for MLE), then
the composite forecast inherits calibration properties from each component, provided the components
are identified (i.e.\ the decomposition is variance-preserving so that the two calibration targets
are orthogonal).
\end{proposition}

\begin{remark}
This is why the two-stage model can achieve excellent binary log-loss \emph{without ever optimizing
binary log-loss directly}. Stage~1's QLIKE ensures honest variance. Stage~2's MLE ensures correct
tail shape. The binary probability inherits both properties through the variance-preserving construction.
In contrast, directly optimizing binary log-loss over both variance and tail parameters simultaneously
creates an adversarial dynamic where improvements in binary log-loss can come from variance distortion
(Remark~\ref{rem:why_not_logloss}).
\end{remark}


% ===================================================================
\section{Statistical Estimation}
\label{sec:estimation}

\subsection{Stage 1 optimization}

The QLIKE objective \eqref{eq:qlike_weighted} is minimized over
$\theta = (c, \alpha, k_0, k_1)$ subject to box constraints:
\begin{equation}
    c \in [0.1,\, 3.0], \quad
    \alpha \in [0.5,\, 1.5], \quad
    k_0 \in [-10,\, 5], \quad
    k_1 \in [-3,\, 3].
    \label{eq:box_constraints}
\end{equation}
We use L-BFGS-B \citep{byrd1995limited}, a quasi-Newton method for bound-constrained optimization,
which requires gradients. The gradient of the QLIKE loss for a single observation is:
\begin{equation}
    \frac{\partial L_{\mathrm{Q}}}{\partial \theta_j}
    = \left(1 - \frac{r_i^2}{\hat{v}_i}\right) \cdot \frac{1}{\hat{v}_i} \cdot \frac{\partial \hat{v}_i}{\partial \theta_j}.
    \label{eq:qlike_grad}
\end{equation}
Write $B := \sigTod^2 \cdot \tau^\alpha$ (the ``base'' variance) and
$f := m + (1-m)\,\sigRel^{2}$ (the shrinkage factor), so $v = c^2 B f$.
The partial derivatives are:
\begin{align}
    \frac{\partial v}{\partial c} &= \frac{2v}{c}, \label{eq:dv_dc} \\
    \frac{\partial v}{\partial \alpha} &= v \cdot \ln\tau. \label{eq:dv_dalpha}
\end{align}
For the shrinkage coefficients, let $P := \sigRel^{2}$ and note
$\partial f / \partial k_j = (1 - P)\,m(1-m)\,\partial z / \partial k_j$, where
$z = k_0 + k_1\ln\tau$. Then:
\begin{equation}
    \frac{\partial v}{\partial k_j} = c^2 B \cdot (1 - P)\,m(1-m) \cdot
    \frac{\partial z}{\partial k_j}, \qquad j = 0,1,
    \label{eq:dv_dk}
\end{equation}
with $\partial z / \partial k_0 = 1$ and $\partial z / \partial k_1 = \ln\tau$.

\subsection{Stage 2 optimization}

The Student-$t$ log-likelihood \eqref{eq:t_loglik} is maximized over $\nu \in [\nu_{\min}, \nu_{\max}]$
(a one-dimensional bounded optimization). The gradient with respect to $\nu$ is:
\begin{equation}
    \frac{\partial \ell}{\partial \nu} = \sum_{i=1}^{N}\left[
    \tfrac{1}{2}\psi\!\left(\tfrac{\nu+1}{2}\right)
    - \tfrac{1}{2}\psi\!\left(\tfrac{\nu}{2}\right)
    - \frac{1}{2\nu}
    - \tfrac{1}{2}\ln\!\left(1 + \frac{w_i^2}{\nu}\right)
    + \frac{(\nu+1)w_i^2}{2\nu(\nu + w_i^2)}
    - \frac{\partial \ln s}{\partial \nu}
    \right],
    \label{eq:dll_dnu}
\end{equation}
where $\psi$ is the digamma function and
\begin{equation}
    \frac{\partial \ln s}{\partial \nu} = \frac{1}{2}\left(\frac{1}{\nu-2} - \frac{1}{\nu}\right)
    = \frac{1}{\nu(\nu-2)}.
\end{equation}
The one-dimensional optimization can be solved efficiently by Brent's method or a bounded Newton step.

\subsection{Cluster-robust standard errors}
\label{sec:standard_errors}

Observations within the same market hour are not independent (they share the same terminal price $S_T$
and are subject to common shocks). Standard errors must account for this clustering.

We use a cluster-level standard error that avoids Hessian computation. For any statistic of interest
(e.g.\ $\bar{u}$ in a diagnostic bin, or a per-observation loss), let $\bar{x}_m$ denote the
within-market mean for market $m$. The cluster-robust SE is:
\begin{equation}
    \mathrm{SE}_{\mathrm{cluster}} = \frac{\mathrm{sd}(\bar{x}_1, \ldots, \bar{x}_M)}{\sqrt{M}},
    \label{eq:cluster_var}
\end{equation}
where $\mathrm{sd}$ denotes the sample standard deviation with Bessel's correction. This treats each
market hour as an independent observation, accounting for the within-market dependence induced by the
shared terminal price $S_T$. For Stage~2, the same approach gives:
\begin{equation}
    \mathrm{SE}(\hat\nu) = \frac{\mathrm{sd}(g_{1,\nu}, \ldots, g_{M,\nu})}{\sqrt{M}},
\end{equation}
where $g_{m,\nu}$ is the per-market mean of the per-observation score $\partial\ell_i/\partial\nu$
evaluated at $\nu = \hat\nu$.


% ===================================================================
\section{Complete Pricer Construction}
\label{sec:pricer}

This section specifies the end-to-end procedure for constructing the pricer from raw data to real-time
probability output.

\subsection{Offline calibration pipeline}
\label{sec:offline}

\begin{enumerate}
    \item \textbf{Data preparation.}
    \begin{enumerate}[nosep, label=(\alph*)]
        \item Obtain tick-level best-bid-offer (BBO) data at 1-second resolution.
        \item For each market hour, record strike $K$ (hour-open mid-price), expiry $T$, and terminal
            price $S_T$ (hour-close mid-price).
        \item Sample calibration rows at regular intervals (e.g.\ every 60 seconds) within each market hour.
            Each row contains: $S_t$, $K$, $\tau = T - t$, $S_T$, and $r_{t,T} = \ln(S_T/S_t)$.
    \end{enumerate}

    \item \textbf{Feature construction.}
    \begin{enumerate}[nosep, label=(\alph*)]
        \item Compute tick-time returns and intervals \eqref{eq:tick_returns} from the BBO stream.
        \item Estimate separate weekday and weekend seasonal curves $\sigTod^{\mathrm{wd}}(b)$,
            $\sigTod^{\mathrm{we}}(b)$ via \eqref{eq:sigma_day_bucket}--\eqref{eq:sigma_tod}
            with circular smoothing.
        \item Compute per-row $\sigRv(t)$ via EWMA \eqref{eq:sigma_rv} with half-life $H$.
        \item For each calibration row, compute integrated seasonal volatility
            $\bar{\sigma}_{\mathrm{tod}}(t,\tau)$ via \eqref{eq:sigma_tod_integrated}
            and $\sigRel(t,\tau) = \sigRv(t)/\bar{\sigma}_{\mathrm{tod}}(t,\tau)$.
    \end{enumerate}

    \item \textbf{Stage 1 calibration.}
    \begin{enumerate}[nosep, label=(\alph*)]
        \item Minimize market-weighted QLIKE \eqref{eq:qlike_weighted} over
            $\theta = (c, \alpha, k_0, k_1)$ with box constraints.
        \item Compute variance forecasts $\hat{v}_i = v_{t_i}(\tau_i;\hat\theta)$ for all calibration rows.
    \end{enumerate}

    \item \textbf{Stage 1 diagnostics.}
    \begin{enumerate}[nosep, label=(\alph*)]
        \item Compute $u_i = r_i^2 / \hat{v}_i$.
        \item Check $\E[u \mid X] \approx 1$ for $X \in \{\tau, \sigRel, \Delta t_{\mathrm{last}}, h_t\}$.
        \item If diagnostics fail, iterate (adjust $\alpha$, re-smooth $\sigTod$, adjust
            shrinkage parameters $k_0, k_1$, or half-life $H$).
    \end{enumerate}

    \item \textbf{Stage 2 calibration.}
    \begin{enumerate}[nosep, label=(\alph*)]
        \item Compute standardized residuals $z_i = r_i / \sqrt{\hat{v}_i}$.
        \item Maximize the Student-$t$ log-likelihood \eqref{eq:t_loglik} over $\nu \in [3.01, 100]$.
        \item Store $\hat\nu$.
    \end{enumerate}

    \item \textbf{Stage 2 diagnostics.}
    \begin{enumerate}[nosep, label=(\alph*)]
        \item QQ-plot of $z_i$ against $t_{\hat\nu}$ quantiles (variance-preserving scaled).
        \item Tail exceedance check: compare empirical $\Prob(|z| > q)$ to model prediction
            $2[1 - T_{\hat\nu}(q / s(\hat\nu))]$ for thresholds $q \in \{2, 3, 4\}$.
    \end{enumerate}
\end{enumerate}

\subsection{Online pricing}
\label{sec:online}

At each quote time $t$ with current mid-price $S_t$, strike $K$, and time to expiry $\tau$:

\begin{enumerate}[nosep]
    \item \textbf{Update features:} Compute $\sigRv(t)$ from the live tick stream via EWMA
        \eqref{eq:sigma_rv}.
    \item \textbf{Integrate seasonal:} Compute $\bar{\sigma}_{\mathrm{tod}}(t,\tau)$ by
        integrating the pre-computed seasonal curve over $[t, t+\tau]$
        (weekday or weekend curve as appropriate).
    \item \textbf{Regime level:} $\sigRel = \sigRv(t) / \bar{\sigma}_{\mathrm{tod}}(t,\tau)$.
    \item \textbf{Shrinkage:} $m = \sigma(k_0 + k_1\ln\tau)$,
        $f = m + (1-m)\,\sigRel^{2}$.
    \item \textbf{Compute variance:} $\hat{v} = c^2 \cdot \bar{\sigma}_{\mathrm{tod}}^2
        \cdot \tau^\alpha \cdot f$.
    \item \textbf{Compute log-strike:} $k = \ln(K/S_t)$.
    \item \textbf{Price (Gaussian):} $p = \Phi\!\left(-k / \sqrt{\hat{v}}\right)$.
    \item \textbf{Price (Student-$t$):} $p = T_{\hat\nu}\!\left(-k\, /\, [s(\hat\nu)\sqrt{\hat{v}}]\right)$.
\end{enumerate}

The entire computation involves elementary operations plus a single CDF evaluation and can be executed
in microseconds.

\subsection{Live monitoring}
\label{sec:monitoring}

The deployed model is monitored via three channels:

\begin{enumerate}[nosep]
    \item \textbf{Scale monitor:} Rolling estimates of $\bar{u} = \E[u]$ and $\E[u \mid X]$ across
        $(\tau, \sigRel, \Delta t_{\mathrm{last}}, h_t)$. Detects when the variance model drifts out of calibration.
    \item \textbf{Tail monitor:} Rolling exceedance rates $\hat{\Prob}(|z| > q)$ compared to
        Student-$t$ predictions. Detects when the tail parameter $\nu$ needs updating.
    \item \textbf{Override knobs:} The parameters $(c, \alpha, k_0, k_1, \nu)$
        have interpretable effects. In an emergency, a trader can:
        \begin{itemize}[nosep]
            \item Increase $c$ to widen the variance forecast globally.
            \item Increase $k_0$ to shrink more aggressively toward the seasonal baseline.
            \item Decrease $\nu$ to make tails heavier.
        \end{itemize}
        Because of the decoupling guarantee (Theorem~\ref{thm:decoupling}), adjusting $\nu$ does not
        affect the variance forecast, and adjusting $(c, \alpha, k_0, k_1)$ does not
        affect the tail parameter.
\end{enumerate}


% ===================================================================
\section{Extensions and Generalizations}
\label{sec:extensions}

\subsection{State-dependent degrees of freedom}

The fixed-$\nu$ model applies uniform tail weight across all market conditions. One could parameterize
$\nu$ as a function of state variables (e.g.\ via a sigmoid link to ensure $\nu \in [\nu_{\min}, \nu_{\max}]$).
The variance-preserving property (Proposition~\ref{prop:var_preserve}) holds observation-by-observation
for any $\nu_i > 2$, so the decoupling guarantee would extend immediately. However, walk-forward
cross-validation on BTC hourly data shows that the additional parameters provide no out-of-sample
improvement over the scalar $\nu$ baseline, so we adopt the fixed-$\nu$ specification.

\subsection{Alternative innovation distributions}

The variance-preserving overlay framework is not specific to the Student-$t$. Any location-scale family
$F_\phi$ with $\E[X] = 0$, $\Var(X) = \sigma^2_\phi$ can be made variance-preserving by scaling:
\begin{equation}
    \varepsilon_t = \frac{X}{\sigma_\phi}, \qquad X \sim F_\phi.
\end{equation}
This yields the pricing formula:
\begin{equation}
    p_t = 1 - F_\phi\!\left(\frac{k_t \cdot \sigma_\phi}{\sqrt{v_t(\tau)}}\right).
\end{equation}
Examples include the normal-inverse Gaussian (NIG) and generalized hyperbolic (GH) families. The choice
of family should be driven by empirical QQ-plot fit and parsimony.

\subsection{Non-zero drift}

If Assumption~\ref{ass:zero_drift} is relaxed (e.g.\ at longer horizons where expected returns become
non-negligible), the pricing formula generalizes to:
\begin{equation}
    p_t = T_\nu\!\left(-\frac{k_t - \mu_t(\tau)}{s(\nu)\sqrt{v_t(\tau)}}\right),
\end{equation}
where $\mu_t(\tau) = \E[r_{t,T} \mid \F_t]$ is a drift forecast. The QLIKE objective targets the
\emph{centered} second moment $\E[(r - \mu)^2 \mid \F_t]$ instead:
\begin{equation}
    \mathrm{QLIKE}_{\text{centered}}(\theta, \mu) = \frac{1}{N}\sum_{i=1}^{N}
    \left[\ln \hat{v}_i + \frac{(r_i - \hat\mu_i)^2}{\hat{v}_i}\right].
\end{equation}
This adds a drift estimation sub-problem, which requires its own proper scoring rule (the squared error
loss is proper for the conditional mean).


% ===================================================================
\section{Summary of Notation}
\label{sec:notation}

\begin{table}[h]
\centering
\caption{Summary of principal symbols.}
\label{tab:notation}
\begin{tabular}{@{}ll@{}}
\toprule
Symbol & Meaning \\
\midrule
$S_t$ & Asset mid-price at time $t$ \\
$K$ & Strike price (hour-open mid-price) \\
$T$ & Contract expiry time \\
$\tau = T - t$ & Time to expiry (seconds) \\
$r_{t,T} = \ln(S_T/S_t)$ & Terminal log-return \\
$k_t = \ln(K/S_t)$ & Log-strike distance \\
$p_t = \Prob(S_T > K \mid \F_t)$ & Binary option fair price \\
\midrule
$B$ & Number of TOD buckets (default $288$ for 5-minute buckets) \\
$W$ & Smoothing half-width for $\sigTod$ (buckets; default $3$) \\
$\sigTod(b)$ & Seasonal volatility curve (per bucket) \\
$\bar{\sigma}_{\mathrm{tod}}(t,\tau)$ & Integrated seasonal vol over $[t, t+\tau]$ \\
$\sigRv(t)$ & EWMA realized volatility \\
$\sigRel(t,\tau) = \sigRv/\bar{\sigma}_{\mathrm{tod}}$ & Relative regime level \\
$H$ & EWMA half-life (seconds; default $600$) \\
$\Delta t_{\mathrm{last}}$ & Time since last mid-price change (seconds) \\
$h_t$ & Hour of day (Eastern Time) \\
\midrule
$v_t(\tau;\theta)$ & Parametric variance forecast \\
$c, \alpha$ & Variance scale, horizon exponent \\
$k_0, k_1$ & Shrinkage parameters (baseline, horizon) \\
$m_t(\tau)$ & State-dependent shrinkage weight \\
$u_i = r_i^2 / \hat{v}_i$ & Normalized squared return (diagnostic) \\
\midrule
$\nu$ & Student-$t$ degrees of freedom (Stage~2) \\
$s(\nu) = \sqrt{(\nu-2)/\nu}$ & Variance-preserving scale \\
$T_\nu(\cdot)$ & Student-$t$ CDF \\
$\Phi(\cdot)$ & Standard normal CDF \\
\midrule
$\mathrm{QLIKE}$ & Proper scoring rule for variance \\
$L_{\mathrm{LL}}$ & Binary log-loss \\
$\ell(\nu)$ & Student-$t$ log-likelihood \\
\bottomrule
\end{tabular}
\end{table}


% ===================================================================
\bibliographystyle{plainnat}
\begin{thebibliography}{9}

\bibitem[Byrd et~al.(1995)]{byrd1995limited}
R.~H.~Byrd, P.~Lu, J.~Nocedal, and C.~Zhu.
\newblock A limited memory algorithm for bound constrained optimization.
\newblock \emph{SIAM Journal on Scientific Computing}, 16(5):1190--1208, 1995.

\bibitem[Gneiting and Raftery(2007)]{gneiting2007strictly}
T.~Gneiting and A.~E.~Raftery.
\newblock Strictly proper scoring rules, prediction, and estimation.
\newblock \emph{Journal of the American Statistical Association}, 102(477):359--378, 2007.

\bibitem[Patton(2011)]{patton2011volatility}
A.~J.~Patton.
\newblock Volatility forecast comparison using imperfect volatility proxies.
\newblock \emph{Journal of Econometrics}, 160(1):246--256, 2011.

\end{thebibliography}

\end{document}
